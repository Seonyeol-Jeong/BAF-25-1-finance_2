{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda76890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.58 (from langchain)\n",
      "  Downloading langchain_core-0.3.60-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.3.42-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Downloading pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading sqlalchemy-2.0.41-cp312-cp312-win_amd64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.58->langchain)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.58->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\chica\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.27.0)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.18-cp312-cp312-win_amd64.whl.metadata (43 kB)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading zstandard-0.23.0-cp312-cp312-win_amd64.whl.metadata (3.0 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading pydantic_core-2.33.2-cp312-cp312-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.2.2-cp312-cp312-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
      "Downloading langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 15.9 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.3.60-py3-none-any.whl (437 kB)\n",
      "Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Downloading langsmith-0.3.42-py3-none-any.whl (360 kB)\n",
      "Downloading pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.2-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 15.5 MB/s eta 0:00:00\n",
      "Downloading sqlalchemy-2.0.41-cp312-cp312-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.1/2.1 MB 13.2 MB/s eta 0:00:00\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading greenlet-3.2.2-cp312-cp312-win_amd64.whl (296 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading orjson-3.10.18-cp312-cp312-win_amd64.whl (134 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading zstandard-0.23.0-cp312-cp312-win_amd64.whl (495 kB)\n",
      "Installing collected packages: zstandard, typing-inspection, tenacity, pydantic-core, orjson, jsonpatch, greenlet, annotated-types, SQLAlchemy, pydantic, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "Successfully installed SQLAlchemy-2.0.41 annotated-types-0.7.0 greenlet-3.2.2 jsonpatch-1.33 langchain-0.3.25 langchain-core-0.3.60 langchain-text-splitters-0.3.8 langsmith-0.3.42 orjson-3.10.18 pydantic-2.11.4 pydantic-core-2.33.2 tenacity-9.1.2 typing-inspection-0.4.0 zstandard-0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "100cace8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.3.25)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-1.79.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.3.60)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.3.42)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.11.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
      "  Downloading aiohttp-3.11.18-cp312-cp312-win_amd64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (9.1.2)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (2.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.4.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (0.27.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.10.0-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.12.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading frozenlist-1.6.0-cp312-cp312-win_amd64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading multidict-6.4.4-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading propcache-0.3.1-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading yarl-1.20.0-cp312-cp312-win_amd64.whl.metadata (74 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\chica\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\chica\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\chica\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.5 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 1.3/2.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 4.7 MB/s eta 0:00:00\n",
      "Downloading openai-1.79.0-py3-none-any.whl (683 kB)\n",
      "   ---------------------------------------- 0.0/683.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 683.3/683.3 kB 9.0 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.11.18-cp312-cp312-win_amd64.whl (439 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading jiter-0.10.0-cp312-cp312-win_amd64.whl (206 kB)\n",
      "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.6.0-cp312-cp312-win_amd64.whl (120 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading multidict-6.4.4-cp312-cp312-win_amd64.whl (38 kB)\n",
      "Downloading propcache-0.3.1-cp312-cp312-win_amd64.whl (44 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading yarl-1.20.0-cp312-cp312-win_amd64.whl (92 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Installing collected packages: propcache, mypy-extensions, multidict, marshmallow, jiter, httpx-sse, frozenlist, distro, aiohappyeyeballs, yarl, typing-inspect, aiosignal, pydantic-settings, openai, dataclasses-json, aiohttp, langchain-community\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 dataclasses-json-0.6.7 distro-1.9.0 frozenlist-1.6.0 httpx-sse-0.4.0 jiter-0.10.0 langchain-community-0.3.24 marshmallow-3.26.1 multidict-6.4.4 mypy-extensions-1.1.0 openai-1.79.0 propcache-0.3.1 pydantic-settings-2.9.1 typing-inspect-0.9.0 yarl-1.20.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install langchain langchain-community openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "202f0740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 데이터 처리 및 입출력\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# LangChain 및 OpenAI API 관련\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a66ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = # api 키"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "440e4e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqdm 초기화:\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98b95acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다수결 방식으로 라벨링을 수행하는 함수: 5번 시행중 3개 이상 일치하는 라벨을 선택\n",
    "def classify_scandal_labels_multi(text):\n",
    "    from collections import Counter\n",
    "\n",
    "    results = []\n",
    "    for _ in range(5):\n",
    "        labels = classify_scandal_labels(text)\n",
    "        results.append(tuple(labels))  # 튜플로 처리하여 Counter에서 집계\n",
    "\n",
    "    # 다수결 방식으로 상위 3개 선택\n",
    "    flat_labels = [label for labels in results for label in labels if pd.notna(label)]\n",
    "    common_labels = [label for label, _ in Counter(flat_labels).most_common(3)]\n",
    "\n",
    "    while len(common_labels) < 3:\n",
    "        common_labels.append(np.nan)\n",
    "\n",
    "    return common_labels[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdf5d6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 처리 후 재 라벨링\n",
    "# 1. CSV 로드\n",
    "df = pd.read_csv(\"C:/Users/chica/OneDrive/바탕 화면/BAF-25-1-finance_2/데이터/스캔들/ALL논란_Final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab59c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 불용어 처리: 사건 내용 정제 + 불용어 제거\n",
    "df.head(3)\n",
    "\n",
    "# 조사\n",
    "particles_to_remove = {\n",
    "    \"은\", \"는\", \"이\", \"가\", \"을\", \"를\", \"에\", \"의\", \"도\", \"과\", \"와\",\n",
    "    \"에서\", \"부터\", \"까지\", \"만\", \"든\", \"로\", \"으로\", \"라도\", \"조차\"\n",
    "}\n",
    "\n",
    "# 한 글자지만 의미가 중요한 단어\n",
    "important_single_chars = {\"팬\", \"군\", \"죄\", \"법\", \"술\", \"약\"}\n",
    "\n",
    "with open('korean_stopwords.txt', encoding='utf-8') as f:\n",
    "    stopwords = set(w.strip() for w in f.readlines() if w.strip())\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # 0. 날짜 표현 제거\n",
    "    text = re.sub(r\"\\d{4}년\\s*\\d{1,2}월\\s*\\d{1,2}일\", \" \", text)\n",
    "    text = re.sub(r\"\\d{4}년\", \" \", text)\n",
    "    text = re.sub(r\"\\d{1,2}월\", \" \", text)\n",
    "    text = re.sub(r\"\\d{1,2}일\", \" \", text)\n",
    "\n",
    "    # 조사에 대한 조건 추가\n",
    "    # 1. 명사 + 조사 형태 제거 (예: 정국은 → 정국)\n",
    "    text = re.sub(r\"([가-힣]+)(으로|에서|부터|까지|라도|조차|은|는|이|가|을|를)\\b\", r\"\\1\", text)\n",
    "    # 과 -> 취재결과 경우로 생략\n",
    "    \n",
    "    # 1. 특수문자 및 한글/숫자/공백 이외 제거\n",
    "    text = re.sub(r\"[^가-힣0-9\\s]\", \" \", text)\n",
    "    \n",
    "    # 2. 중복 공백 제거 및 좌우 공백 제거\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    # 3. 불용어 목록에 있는 단어 제거\n",
    "    tokens = []\n",
    "    for t in text.split():\n",
    "        if t in stopwords:\n",
    "            continue\n",
    "        if t in particles_to_remove:\n",
    "            continue\n",
    "        if len(t) == 1 and t not in important_single_chars:\n",
    "            continue\n",
    "        tokens.append(t)\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df['clean_text'] = df['사건 내용'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cab11241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>소속사</th>\n",
       "      <th>그룹</th>\n",
       "      <th>연예인 이름</th>\n",
       "      <th>사건 날짜</th>\n",
       "      <th>사건 내용</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HYBE</td>\n",
       "      <td>방탄소년단</td>\n",
       "      <td>지민</td>\n",
       "      <td>2022-04-25</td>\n",
       "      <td>2022년 4월 25일, 언론사 비즈한국의 취재결과 건강보험료 체납으로 보유하고 있...</td>\n",
       "      <td>언론사 비즈한국의 취재결과 건강보험료 체납 보유하고 있던 아파트 압류당한 사실 뒤늦...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HYBE</td>\n",
       "      <td>방탄소년단</td>\n",
       "      <td>정국</td>\n",
       "      <td>2024-01-06</td>\n",
       "      <td>2024년 1월 6일, 정국이 입대 후 신병 교육을 받는 동안 발생했다. 탈취범은 ...</td>\n",
       "      <td>정국 입대 신병 교육 발생했다 탈취범 정국의 동의 정국 명의로 3개의 증권 계좌 무...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HYBE</td>\n",
       "      <td>엔하이픈</td>\n",
       "      <td>정원</td>\n",
       "      <td>2021-11-18</td>\n",
       "      <td>2021년 11월 18일 오후 5시 , ENHYPEN 은 D-1 이라는 제목으로 V...</td>\n",
       "      <td>오후 5시 이라 제목 진행하였다 자리 개최되 번째 팬미팅에 소식 전했다 그렇게 팬들...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    소속사     그룹 연예인 이름       사건 날짜  \\\n",
       "0  HYBE  방탄소년단     지민  2022-04-25   \n",
       "1  HYBE  방탄소년단     정국  2024-01-06   \n",
       "2  HYBE   엔하이픈     정원  2021-11-18   \n",
       "\n",
       "                                               사건 내용  \\\n",
       "0  2022년 4월 25일, 언론사 비즈한국의 취재결과 건강보험료 체납으로 보유하고 있...   \n",
       "1  2024년 1월 6일, 정국이 입대 후 신병 교육을 받는 동안 발생했다. 탈취범은 ...   \n",
       "2  2021년 11월 18일 오후 5시 , ENHYPEN 은 D-1 이라는 제목으로 V...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  언론사 비즈한국의 취재결과 건강보험료 체납 보유하고 있던 아파트 압류당한 사실 뒤늦...  \n",
       "1  정국 입대 신병 교육 발생했다 탈취범 정국의 동의 정국 명의로 3개의 증권 계좌 무...  \n",
       "2  오후 5시 이라 제목 진행하였다 자리 개최되 번째 팬미팅에 소식 전했다 그렇게 팬들...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52f5c5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chica\\AppData\\Local\\Temp\\ipykernel_19548\\2925259079.py:55: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm=ChatOpenAI(\n",
      "C:\\Users\\chica\\AppData\\Local\\Temp\\ipykernel_19548\\2925259079.py:54: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(\n",
      "100%|██████████| 363/363 [09:08<00:00,  1.51s/it]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ALL논란_Final+라벨링.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 97\u001b[0m\n\u001b[0;32m     92\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFianl.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m###############\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# 누락된 라벨 재 라벨링\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m###############\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mALL논란_Final+라벨링.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# 1. 모든 라벨이 비어 있는 행만 필터링\u001b[39;00m\n\u001b[0;32m    100\u001b[0m mask_missing \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m라벨 1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m라벨 2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m라벨 3\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39mall(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\chica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\chica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\chica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ALL논란_Final+라벨링.csv'"
     ]
    }
   ],
   "source": [
    "# 2. 라벨링 기준 정의\n",
    "scan_labels = {\n",
    "    \"병역 문제\": \"군 입대, 대체복무 병역 면제 논란\",\n",
    "    \"범죄 혐의\": \"마약, 음주운전, 폭행, 협박 등\",\n",
    "    \"세금 문제\": \"세금 누락, 탈세, 투자 비리 등\",\n",
    "    \"성 관련\": \"성희롱, 성폭력, 사생활 유포 등\",\n",
    "    \"사생활\": \"열애설, 혼전 임신, 연애 등\",\n",
    "    \"팬 대응\": \"팬 무시, 사생 대응 발언 등\",\n",
    "    \"발언 문제\": \"SNS 발언, 부적절 표현\",\n",
    "    \"사회적 감수성\": \"성차별, 인종차별 발언 등\",\n",
    "    \"종교/이념\": \"정치/종교적 편향 발언\",\n",
    "    \"혐의정보 유포\": \"사이버 명예훼손/혐의 영상 유포\",\n",
    "    \"무의식적 태도\": \"방송 중 실언, 무례함 등\",\n",
    "    \"기타\": \"분류 어려운 사건 또는 해프닝\",\n",
    "    \"무혐의\": \"혐의 없음, 무죄 판결\"\n",
    "}\n",
    "\n",
    "label_criteria = \"\\n\".join([f\"{k}: {v}\" for k, v in scan_labels.items()])\n",
    "label_names = \", \".join(scan_labels.keys())\n",
    "\n",
    "# 3. LangChain 프롬프트 구성 (chat_history 제거 버전)\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "         \"\"\"\n",
    "당신은 한국 연예인의 다양한 이슈 및 사건을 특정 카테고리로 분류하는 분류 전문가입니다.\n",
    "\n",
    "이슈란 다음과 같은 모든 내용을 포함합니다:\n",
    "사건, 논란, 의혹, 발표, 활동, 해명, 논평, 입대, 제대, 결혼 발표 등 사회적으로 주목받은 일들입니다.\n",
    "단순 사실이나 발표도 해당 카테고리에 포함시켜 주세요. 논란만을 분류 대상으로 보지 마세요.\n",
    "\n",
    "아래는 당신이 사용할 라벨 목록과 각 라벨의 기준입니다:\n",
    "\n",
    "{label_criteria}\n",
    "\n",
    "---\n",
    "\n",
    "기사 본문:\n",
    "\"{text}\"\n",
    "\n",
    "---\n",
    "\n",
    "위 기사를 읽고 아래 라벨 목록 중에서 가장 관련 있는 라벨을 최대 3개까지 콤마(,)로 구분하여 출력하세요.\n",
    "\n",
    "- 꼭 관련된 라벨만 골라야 하며, 라벨명 외에는 아무것도 출력하지 마세요.\n",
    "- 하나도 해당되지 않는 경우에만 'nan' 또는 'None' 중 하나로만 답하세요. 애매하거나 확신이 없다면 라벨을 선택하지마세요.\n",
    "\n",
    "사용 가능한 라벨 목록:\n",
    "{label_names}\n",
    "        \"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "# 4. LLMChain 구성\n",
    "llm_chain = LLMChain(\n",
    "    llm=ChatOpenAI(\n",
    "        temperature=0,\n",
    "        max_tokens=512,\n",
    "        model_name=\"gpt-4o\"\n",
    "    ),\n",
    "    prompt=prompt_template\n",
    ")\n",
    "\n",
    "# 5. 라벨 추출 함수\n",
    "def classify_scandal_labels(text):\n",
    "    try:\n",
    "        response = llm_chain.invoke({\n",
    "            \"text\": text,\n",
    "            \"label_criteria\": label_criteria,\n",
    "            \"label_names\": label_names\n",
    "        })\n",
    "        result = response['text'].strip()\n",
    "\n",
    "        # nan 처리\n",
    "        if result.lower() in ['none', 'nan', '해당 없음']:\n",
    "            return [np.nan, np.nan, np.nan]\n",
    "\n",
    "        # 유효한 라벨만 추출 (최대 3개)\n",
    "        labels = [l.strip() for l in result.split(\",\") if l.strip() in scan_labels]\n",
    "        labels = labels[:3]\n",
    "        while len(labels) < 3:\n",
    "            labels.append(np.nan)\n",
    "        return labels\n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {e}\")\n",
    "        return [np.nan, np.nan, np.nan]\n",
    "\n",
    "# 6. 라벨링 실행\n",
    "tqdm.pandas()\n",
    "df[['라벨 1', '라벨 2', '라벨 3']] = df['clean_text'].progress_apply(classify_scandal_labels).apply(pd.Series)\n",
    "\n",
    "# 7. 결과 저장\n",
    "df.to_csv(\"Fianl.csv\", index=False)\n",
    "\n",
    "###############\n",
    "# 누락된 라벨 재 라벨링\n",
    "###############\n",
    "df = pd.read_csv(\"ALL논란_Final+라벨링.csv\")\n",
    "\n",
    "# 1. 모든 라벨이 비어 있는 행만 필터링\n",
    "mask_missing = df[['라벨 1', '라벨 2', '라벨 3']].isnull().all(axis=1)\n",
    "df_missing = df[mask_missing].copy()\n",
    "\n",
    "# 2. index 정렬 보존용 열 추가\n",
    "df_missing['__original_index__'] = df_missing.index\n",
    "\n",
    "# 3. 라벨링 재시도 (5회 다수결)\n",
    "df_missing[['라벨 1', '라벨 2', '라벨 3']] = df_missing['clean_text'].progress_apply(classify_scandal_labels_multi).apply(pd.Series)\n",
    "\n",
    "# 4. index 복원 후 원본 df에 반영\n",
    "for idx, row in df_missing.iterrows():\n",
    "    df.loc[row['__original_index__'], ['라벨 1', '라벨 2', '라벨 3']] = row[['라벨 1', '라벨 2', '라벨 3']]\n",
    "\n",
    "# 5. 저장\n",
    "df.to_csv(\"ALL논란_Final+라벨링.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ef1ef99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [10:41<00:00,  6.61s/it]\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "# 누락된 라벨 재 라벨링\n",
    "###############\n",
    "df = pd.read_csv(\"Fianl.csv\")\n",
    "\n",
    "# 1. 모든 라벨이 비어 있는 행만 필터링\n",
    "mask_missing = df[['라벨 1', '라벨 2', '라벨 3']].isnull().all(axis=1)\n",
    "df_missing = df[mask_missing].copy()\n",
    "\n",
    "# 2. index 정렬 보존용 열 추가\n",
    "df_missing['__original_index__'] = df_missing.index\n",
    "\n",
    "# 3. 라벨링 재시도 (5회 다수결)\n",
    "df_missing[['라벨 1', '라벨 2', '라벨 3']] = df_missing['clean_text'].progress_apply(classify_scandal_labels_multi).apply(pd.Series)\n",
    "\n",
    "# 4. index 복원 후 원본 df에 반영\n",
    "for idx, row in df_missing.iterrows():\n",
    "    df.loc[row['__original_index__'], ['라벨 1', '라벨 2', '라벨 3']] = row[['라벨 1', '라벨 2', '라벨 3']]\n",
    "\n",
    "# 5. 저장\n",
    "df.to_csv(\"ALL논란_Final+라벨링.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44173fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 라벨이 비어 있는 행 수: 87\n"
     ]
    }
   ],
   "source": [
    "# 모든 행이 비어 있는 경우\n",
    "mask_all_nan = df[['라벨 1', '라벨 2', '라벨 3']].isnull().all(axis=1)\n",
    "df_all_nan = df[mask_all_nan]\n",
    "print(f\"모든 라벨이 비어 있는 행 수: {len(df_all_nan)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b9d16a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하나 이상 라벨이 비어 있는 행 수: 307\n"
     ]
    }
   ],
   "source": [
    "# 하나 이상의 라벨이 비어 있는 경우\n",
    "mask_any_nan = df[['라벨 1', '라벨 2', '라벨 3']].isnull().any(axis=1)\n",
    "df_any_nan = df[mask_any_nan]\n",
    "print(f\"하나 이상 라벨이 비어 있는 행 수: {len(df_any_nan)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
